{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def get_db_config():\n",
    "    \"\"\"Get database configuration from environment variables.\"\"\"\n",
    "    return {\n",
    "        'host': os.getenv('DB_HOST', 'localhost'),\n",
    "        'user': os.getenv('DB_USER'),\n",
    "        'password': os.getenv('DB_PASSWORD'),\n",
    "        'database': os.getenv('DB_NAME'),\n",
    "        'port': int(os.getenv('DB_PORT', '3306')),\n",
    "        'charset': os.getenv('DB_CHARSET', 'utf8mb4'),\n",
    "        'collation': os.getenv('DB_COLLATION', 'utf8mb4_general_ci')\n",
    "    }\n",
    "\n",
    "def create_connection():\n",
    "    \"\"\"Create a database connection from .env configuration.\"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        db_config = get_db_config()\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        if connection.is_connected():\n",
    "            print(f\"Successfully connected to MariaDB at {db_config['host']}\")\n",
    "            print(f\"Connected to server version {connection.get_server_info()}\")\n",
    "            print(f\"Database: {db_config['database']}\")\n",
    "            return connection\n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to MariaDB: {e}\")\n",
    "        print(\"\\nConnection details (excluding password):\")\n",
    "        safe_config = {k: v for k, v in get_db_config().items() if k != 'password'}\n",
    "        print(safe_config)\n",
    "        return None\n",
    "\n",
    "def list_tables():\n",
    "    \"\"\"List all available tables in the database.\"\"\"\n",
    "    connection = create_connection()\n",
    "    if not connection:\n",
    "        print(\"Failed to connect to database\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SHOW TABLES\")\n",
    "        tables = cursor.fetchall()\n",
    "        \n",
    "        if not tables:\n",
    "            print(\"No tables found in the database\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\nAvailable tables:\")\n",
    "        for i, table in enumerate(tables, 1):\n",
    "            print(f\"{i}. {table[0]}\")\n",
    "        \n",
    "        return [table[0] for table in tables]\n",
    "    \n",
    "    except Error as e:\n",
    "        print(f\"Error listing tables: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Database connection closed\")\n",
    "\n",
    "def download_table(table_name, output_format='csv'):\n",
    "    \"\"\"Download an entire table from MariaDB.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table to download\n",
    "        output_format: Format of the output file ('csv' or 'pandas')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame if output_format is 'pandas', otherwise None\n",
    "    \"\"\"\n",
    "    connection = create_connection()\n",
    "    if not connection:\n",
    "        print(\"Failed to connect to database\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        \n",
    "        # Check if table exists\n",
    "        cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "        if not cursor.fetchone():\n",
    "            print(f\"Table '{table_name}' does not exist in the database\")\n",
    "            return None\n",
    "        \n",
    "        # Get column names\n",
    "        cursor.execute(f\"DESCRIBE {table_name}\")\n",
    "        columns = [column['Field'] for column in cursor.fetchall()]\n",
    "        \n",
    "        # Get all data\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        if not rows:\n",
    "            print(f\"The table '{table_name}' is empty\")\n",
    "            return None\n",
    "        \n",
    "        # Count rows\n",
    "        cursor.execute(f\"SELECT COUNT(*) as count FROM {table_name}\")\n",
    "        row_count = cursor.fetchone()['count']\n",
    "        print(f\"Downloaded {row_count} rows from '{table_name}'\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        if output_format == 'csv':\n",
    "            filename = f\"{table_name}.csv\"\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Data saved to {filename}\")\n",
    "            return None\n",
    "        else:\n",
    "            return df\n",
    "    \n",
    "    except Error as e:\n",
    "        print(f\"Error downloading table: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - no need for command-line arguments\n",
    "if __name__ == \"__main__\":\n",
    "    # List all tables\n",
    "    print(\"Listing all tables:\")\n",
    "    tables = list_tables()\n",
    "    \n",
    "    # Download the embeddings table\n",
    "    print(\"\\nDownloading 'embeddings' table:\")\n",
    "    download_table(\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def get_db_config():\n",
    "    \"\"\"Get database configuration from environment variables.\"\"\"\n",
    "    return {\n",
    "        'host': os.getenv('DB_HOST', 'localhost'),\n",
    "        'user': os.getenv('DB_USER'),\n",
    "        'password': os.getenv('DB_PASSWORD'),\n",
    "        'database': os.getenv('DB_NAME'),\n",
    "        'port': int(os.getenv('DB_PORT', '3306')),\n",
    "        'charset': os.getenv('DB_CHARSET', 'utf8mb4'),\n",
    "        'collation': os.getenv('DB_COLLATION', 'utf8mb4_general_ci')\n",
    "    }\n",
    "\n",
    "def create_connection():\n",
    "    \"\"\"Create a database connection from .env configuration.\"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        db_config = get_db_config()\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        if connection.is_connected():\n",
    "            print(f\"Successfully connected to MariaDB at {db_config['host']}\")\n",
    "            print(f\"Connected to server version {connection.get_server_info()}\")\n",
    "            print(f\"Database: {db_config['database']}\")\n",
    "            return connection\n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to MariaDB: {e}\")\n",
    "        print(\"\\nConnection details (excluding password):\")\n",
    "        safe_config = {k: v for k, v in get_db_config().items() if k != 'password'}\n",
    "        print(safe_config)\n",
    "        return None\n",
    "\n",
    "def upload_csv_to_db(csv_file_path, table_name, if_exists='replace', chunk_size=None):\n",
    "    \"\"\"Upload a CSV file to a MariaDB table with improved handling for large tables.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path: Path to the CSV file\n",
    "        table_name: Name of the table to create or use\n",
    "        if_exists: What to do if the table already exists ('replace', 'append', 'fail')\n",
    "        chunk_size: Process CSV in chunks of this size (None to load entire file)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    connection = create_connection()\n",
    "    if not connection:\n",
    "        print(\"Failed to connect to database\")\n",
    "        return False\n",
    "    \n",
    "    cursor = None\n",
    "    try:\n",
    "        # First, sample the data to understand its structure\n",
    "        # Read a sample to analyze column types\n",
    "        sample_size = 1000  # Adjust based on your data size\n",
    "        sample_df = pd.read_csv(csv_file_path, nrows=sample_size)\n",
    "        \n",
    "        print(f\"Detected {len(sample_df.columns)} columns in CSV file\")\n",
    "        \n",
    "        # Create a dictionary to store column types and a mapping for column names\n",
    "        column_types = {}\n",
    "        column_name_mapping = {}\n",
    "        \n",
    "        # Create mapping from original column names to clean column names\n",
    "        for column in sample_df.columns:\n",
    "            clean_column = ''.join(e for e in column if e.isalnum() or e == '_')\n",
    "            column_name_mapping[column] = clean_column\n",
    "        \n",
    "        # Rename columns in the sample dataframe\n",
    "        sample_df.rename(columns=column_name_mapping, inplace=True)\n",
    "        \n",
    "        # Preprocess the sample data to get a better understanding of column types\n",
    "        for column in sample_df.columns:\n",
    "            # Check if column contains any non-numeric values\n",
    "            if sample_df[column].dtype == 'object':\n",
    "                # If it's a string column, check if it could be numeric\n",
    "                try:\n",
    "                    # Try to convert to numeric\n",
    "                    pd.to_numeric(sample_df[column], errors='raise')\n",
    "                    # If successful, it's likely a numeric column\n",
    "                    column_types[column] = \"FLOAT\"\n",
    "                except:\n",
    "                    # If conversion fails, it contains non-numeric values\n",
    "                    column_types[column] = \"TEXT\"\n",
    "            elif pd.api.types.is_integer_dtype(sample_df[column].dtype):\n",
    "                column_types[column] = \"INT\"\n",
    "            elif pd.api.types.is_float_dtype(sample_df[column].dtype):\n",
    "                column_types[column] = \"FLOAT\"\n",
    "            elif pd.api.types.is_bool_dtype(sample_df[column].dtype):\n",
    "                column_types[column] = \"BOOLEAN\"\n",
    "            elif pd.api.types.is_datetime64_any_dtype(sample_df[column].dtype):\n",
    "                column_types[column] = \"DATETIME\"\n",
    "            else:\n",
    "                column_types[column] = \"TEXT\"\n",
    "        \n",
    "        # Now, process the entire file or in chunks\n",
    "        if chunk_size:\n",
    "            # Process in chunks\n",
    "            df_iterator = pd.read_csv(csv_file_path, chunksize=chunk_size)\n",
    "            first_chunk = next(df_iterator)\n",
    "            # Rename columns in first chunk\n",
    "            first_chunk.rename(columns=column_name_mapping, inplace=True)\n",
    "            df_chunks = [first_chunk]\n",
    "            \n",
    "            # Rename columns in remaining chunks\n",
    "            remaining_chunks = []\n",
    "            for chunk in df_iterator:\n",
    "                chunk.rename(columns=column_name_mapping, inplace=True)\n",
    "                remaining_chunks.append(chunk)\n",
    "            \n",
    "            df_chunks.extend(remaining_chunks)\n",
    "        else:\n",
    "            # Read the entire CSV file\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            print(f\"Read {len(df)} rows from {csv_file_path}\")\n",
    "            # Rename columns in the full dataframe\n",
    "            df.rename(columns=column_name_mapping, inplace=True)\n",
    "            df = df.replace({np.nan: None})\n",
    "        \n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Check if table exists\n",
    "        cursor.execute(f\"SHOW TABLES LIKE '{table_name}'\")\n",
    "        table_exists = cursor.fetchone() is not None\n",
    "        \n",
    "        if table_exists:\n",
    "            if if_exists == 'fail':\n",
    "                print(f\"Table '{table_name}' already exists. Aborting.\")\n",
    "                return False\n",
    "            elif if_exists == 'replace':\n",
    "                print(f\"Dropping existing table '{table_name}'\")\n",
    "                cursor.execute(f\"DROP TABLE {table_name}\")\n",
    "                connection.commit()\n",
    "                table_exists = False\n",
    "        \n",
    "        # Create table if it doesn't exist\n",
    "        if not table_exists:\n",
    "            # Use the column types we detected earlier\n",
    "            column_defs = []\n",
    "            \n",
    "            # Process each column to create the table definition\n",
    "            for column in sample_df.columns:\n",
    "                sql_type = column_types.get(column, \"TEXT\")  # Default to TEXT if not found\n",
    "                \n",
    "                # Special handling for MoveIn column which caused the error in the past\n",
    "                if column == 'MoveIn':\n",
    "                    sql_type = \"TEXT\"  # Force TEXT for MoveIn\n",
    "                \n",
    "                column_defs.append(f\"`{column}` {sql_type}\")\n",
    "            \n",
    "            # Create table\n",
    "            create_table_sql = f\"CREATE TABLE {table_name} ({', '.join(column_defs)})\"\n",
    "            print(f\"Creating table with SQL: {create_table_sql}\")\n",
    "            cursor.execute(create_table_sql)\n",
    "            connection.commit()\n",
    "            print(f\"Table '{table_name}' created successfully\")\n",
    "        \n",
    "        # Insert data\n",
    "        def insert_dataframe(df_to_insert):\n",
    "            # Prepare the SQL placeholders and column names\n",
    "            placeholders = ', '.join(['%s'] * len(df_to_insert.columns))\n",
    "            columns = ', '.join(f'`{col}`' for col in df_to_insert.columns)\n",
    "            \n",
    "            insert_sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "            \n",
    "            # Convert DataFrame to list of tuples\n",
    "            values = df_to_insert.replace({np.nan: None}).values.tolist()\n",
    "            \n",
    "            # Execute in batches to avoid memory issues with large datasets\n",
    "            batch_size = 1000\n",
    "            for i in range(0, len(values), batch_size):\n",
    "                batch = values[i:i+batch_size]\n",
    "                cursor.executemany(insert_sql, batch)\n",
    "                connection.commit()\n",
    "                print(f\"Inserted batch {i//batch_size + 1} ({len(batch)} rows)\")\n",
    "        \n",
    "        # Insert data from chunks or entire dataframe\n",
    "        if chunk_size:\n",
    "            total_rows = 0\n",
    "            for i, chunk in enumerate(df_chunks):\n",
    "                insert_dataframe(chunk)\n",
    "                total_rows += len(chunk)\n",
    "                print(f\"Processed chunk {i+1} with {len(chunk)} rows\")\n",
    "            print(f\"Successfully uploaded {total_rows} rows to table '{table_name}'\")\n",
    "        else:\n",
    "            insert_dataframe(df)\n",
    "            print(f\"Successfully uploaded {len(df)} rows to table '{table_name}'\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Error as e:\n",
    "        print(f\"Error uploading CSV to database: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"General error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection and connection.is_connected():\n",
    "            connection.close()\n",
    "            print(\"Database connection closed\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = r\"d:\\OneDrive - Green Energy\\Desktop\\properties_202503181522.csv\"  # Replace with your CSV file path\n",
    "    table_name = \"property\"  # Replace with your desired table name\n",
    "    upload_csv_to_db(csv_file, table_name, chunk_size=50)  # Process in chunks of 50 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
